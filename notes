Lecture 1: Searching
Searching consist in finding a solution to a problem. 
  - State: configuration of a agent and and enviroment (the configuration of a chess board)
  - Initial state
  - Actions: posible choices that can be made in a state
  - Goal test: condition that determines if the problem is solved
  - Path cost: numerical cost associated to a path
  - Solution: sequence of actions that leads to the goal
  - Optimal solution: the solution with the lowest path cost

Data is stored in a node (state, parent node, action, path cost). To search all the diferent
nodes we use a frontier. Contains an initial stat and empty ser of explored items and repeats: 
   1. If the frontier is empty -> stop, No solution
   2. Remove a node from the frontier
   3. If the node==goal state -> stop, Return solution
   4. else expand the node with posibles nodes taht can be reached with a action
      and add the them to the frontier

Methods of search
  - DEPTH-FIRST SEARCH (DFS)
    
  - BREADTH-FIRST SEARCH (BFS)



------------------------------------------------------------------------------------------------------
Lecture 2: Knowing
Representing information and drawing inferences from it
Propositional Logic
    - NOT (¬): inverses the truth of the proposition
    - AND (^): Only when both propositions are TRUE the final is TRUE
    - OR (v): If 1 of 2 propositions is TRUE the final is TRUE
    - IMPLICATION (->): If P happens then Q: Only FALSE whe P(true) and Q(false)
    - BICONDITIONAL (<->): If Q is the same as P then TRUE (both TRUE or both FALSE)
    - MODEL: asigns a T/F value to the propositions. The number of models are 2^n
    - ENTAILMENT (alpha |= beta): in every model where alpha is TRUE then beta is also TRUE
    - Knowledge base: agrupation of all propositions and information of them

INFERENCE: deriving new sentences from the previous
Algorithms
    - MODEL CHECKING
        1. enumerate all possible model
        2. if in every model KB is true, alpha true -> entails true
        3. else -> entails not true

------------------------------------------------------------------------------------------------------
Lecture 3: Uncertainty
Uncertainty is the lack of knowledge about the world. We have possible worlds(w) 
    - PROBABILITY P(w): measure of uncertainty. 0 = impossible, 1 = certain
    - CONDITIONAL PROBABILITY P(a|b): probability of A given B(we know). P(A|B) = P(A^B)/P(B)
    - RANDOM VARIABLE x: variable that can take different values. P(X) = <x1, x2, x3...> 
    - INDEPENDENCE: P(A|B) = P(A) if A and B are independent.
                    P(A^B) = P(A) * P(B|A) = P(A)*P(B) if A and B are independent.
    - BAYES THEOREM: P(A|B) = P(B|A) * P(A) / P(B)
    - JOINT PROBABILITY: probability distribution of a set of random variables.

    -Negation: P(¬A) = 1 - P(A)
    -Or(Inclusion-exclusion formula): P(A v B) = P(A) + P(B) - P(A^B)
    -Marginalization: P(A) = P(A, B) + P(A, ¬B)                    P(X=x_i)=∑_j P(X=x_i, Y=y_j)
    -conditioning: P(X=x_i|Y=y_j)=P(X=x_i, Y=y_j)*P(Y=y_j)


Bayesian network: data structure that represents tehe dependencies among random variables (directec graph)
    - Nodes: random variables
    - arrow: dependencies between nodes (directed edges)
    - Conditional probability table: P(X|parents(X)) for each node X
    - Joint probability distribution: P(X1, X2, ..., Xn) = ∏ P(Xi|parents(Xi))

    Inference: computing the probability of a set of variables given the values of other variables.
    - query: set of variables we want to infer the probability of.
    - evidence: set of variables we know the values of.
    - hidden variables: variables we dont know the values of.
    - Goal: calulate P(X|evidence) for a set of variables X.

python library pomegranate
 

